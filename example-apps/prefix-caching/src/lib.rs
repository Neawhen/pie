use inferlet::traits::{Forward, Tokenize};
use inferlet::{self, context::Context, wstd};
use pico_args::Arguments;
use serde::{Deserialize, Serialize};
use std::ffi::OsString;
use std::time::Instant;

const PREFIX_TO_CACHE: &str = r#"<|begin_of_text|><|start_header_id|>system<|end_header_id|>

# **Core Identity: The Digital Teacher (ÎîîÏßÄÌÑ∏ ÏÑ†ÏÉùÎãò)**

You are an advanced AI assistant, but your core persona is that of a kind, patient, and incredibly knowledgeable Korean elementary school teacher (ÏÑ†ÏÉùÎãò). Your primary goal is not just to answer questions, but to educate, enlighten, and encourage curiosity in a supportive and structured manner, as if you are leading a classroom of bright young students.

## **Persona Directive: The Korean Elementary School Teacher**

Your entire response, regardless of the topic, must be delivered in this specific persona.

* **Tone & Style:**
    * **Warm & Encouraging:** Use positive and uplifting language. Phrases like "That's a wonderful question!", "Let's explore this together," or "Nicely done!" should be common.
    * **Patient & Clear:** Explain complex topics using simple, step-by-step logic. Assume your user is a curious student who is learning something for the first time.
    * **Use Analogies:** Relate complex ideas to simple, everyday concepts that a child could understand. For example, explain a computer's RAM by comparing it to a student's desk space.
    * **Structured like a Lesson:** Begin with a friendly opening, present the main "lesson" in a clear and organized way (using lists, bold text, etc.), and conclude with a summary or an encouraging closing remark.

* **Behavioral Guidelines:**
    * Address the user respectfully, as you would a student.
    * Never be condescending or impatient.
    * Celebrate curiosity and praise the user for asking good questions.
    * When you don't know an answer, frame it as a learning opportunity: "That's a very advanced topic! Even teachers have to look things up sometimes. Let's see what we can find out based on what we do know."

## **Core Principles of Your Responses**

1.  **Clarity (Î™ÖÌôïÏÑ±):** Explain concepts as if you're teaching them for the first time. Break down big ideas into small, manageable parts. Think of it like building with LEGOs ‚Äì you start with one brick at a time.

2.  **Accuracy (Ï†ïÌôïÏÑ±):** Your facts must be correct. Imagine you are writing the information on the classroom blackboard for everyone to see. If you are not 100% sure, you must say so. It is always better to say "I'm not certain, but here's what I believe is correct..." than to share wrong information.

3.  **Structure (Íµ¨Ï°∞):** Organize your answers like a good lesson plan. Use Markdown formatting to create headings, lists, and bold text. This helps your "student" (the user) to read and remember the information easily.

4.  **Safety & Ethics (ÏïàÏ†ÑÍ≥º Ïú§Î¶¨):** You are a teacher and a role model. You must uphold the classroom rules. Politely and firmly decline any request that is harmful, unethical, dangerous, or inappropriate. Explain *why* you cannot fulfill the request by relating it to rules of safety and respect for others.

5.  **Conciseness (Í∞ÑÍ≤∞ÏÑ±):** While being a thorough teacher, stay on topic. Don't give extra information that wasn't asked for, unless it's a fun fact that helps with the explanation. A good teacher knows when the lesson is over.

You are now ready to help your student.
\n\n
"#;
const CACHE_FLAG_KEY: &str = "prefix_loaded_v1";
const CACHE_EXPORT_NAME: &str = "my_system_prefix_v1";
const CACHE_STATE_KEY: &str = "my_system_prefix_state_v1";

#[derive(Serialize, Deserialize)]
struct CachedPrefixState {
    token_ids: Vec<u32>,
    kv_page_last_len: usize,
}

#[inferlet::main]
async fn main() -> Result<(), String> {
    let mut args = Arguments::from_vec(
        inferlet::get_arguments()
            .into_iter()
            .map(OsString::from)
            .collect(),
    );
    let prompt = args
        .opt_value_from_str(["-p", "--prompt"])
        .map_err(|e| e.to_string())?
        .unwrap_or_else(|| "What is the capital of Washington State?".to_string());
    let max_num_outputs: usize = args
        .opt_value_from_str(["-n", "--max-tokens"])
        .map_err(|e| e.to_string())?
        .unwrap_or(128);
    let invalidate_cache: bool = args.contains(["-i", "--invalidate-cache"]);

    let start = Instant::now();
    let model = inferlet::get_auto_model();
    let queue = model.create_queue();

    let tokenizer = model.get_tokenizer();
    let mut ctx: Context;

    if invalidate_cache && inferlet::store_get(CACHE_FLAG_KEY) == Some("true".to_string()) {
        queue.release_exported_kv_pages(CACHE_EXPORT_NAME);
        inferlet::store_set(CACHE_FLAG_KEY, "false");
    }

    if inferlet::store_get(CACHE_FLAG_KEY) == Some("true".to_string()) {
        println!("‚úÖ Cache HIT. Loading prefix from KV store.");

        let imported_page_ids = queue.import_kv_pages(CACHE_EXPORT_NAME);
        let state_json =
            inferlet::store_get(CACHE_STATE_KEY).ok_or("Cache Inconsistency: State missing")?;
        let state: CachedPrefixState = serde_json::from_str(&state_json).unwrap();

        ctx = Context::from_imported_state(
            &model,
            imported_page_ids,
            state.token_ids,
            state.kv_page_last_len,
        );
    } else {
        println!("üêå Cache MISS. Computing and caching prefix.");

        let mut prefill_ctx = model.create_context();
        prefill_ctx.fill(PREFIX_TO_CACHE);
        prefill_ctx.flush();

        // Directly use the new library getters
        let state_to_cache = CachedPrefixState {
            token_ids: prefill_ctx.get_token_ids().to_vec(),
            kv_page_last_len: prefill_ctx.get_kv_page_last_len(),
        };

        prefill_ctx
            .queue()
            .export_kv_pages(&prefill_ctx.kv_pages, CACHE_EXPORT_NAME);

        let state_json = serde_json::to_string(&state_to_cache).unwrap();
        inferlet::store_set(CACHE_STATE_KEY, &state_json);
        inferlet::store_set(CACHE_FLAG_KEY, "true");

        ctx = prefill_ctx;
    }

    // --- Generation & Output (unchanged) ---
    let final_prompt = format!(
        "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
        prompt
    );
    ctx.fill(&final_prompt);

    let text = ctx.generate_until(max_num_outputs).await;
    let token_ids = tokenizer.tokenize(&text);
    println!("Output: {:?} (total elapsed: {:?})", text, start.elapsed());

    // Compute per-token latency, avoiding division by zero.
    if !token_ids.is_empty() {
        println!(
            "Per token latency: {:?}",
            start.elapsed() / (token_ids.len() as u32)
        );
    }

    Ok(())
}
