# ğŸš€ PIE Simple Test å¤šå®ä¾‹åŸºå‡†æµ‹è¯• - å¿«é€Ÿå¼€å§‹æŒ‡å—

## å‰ç½®è¦æ±‚

1. **å®‰è£… Rust**ï¼š
   ```bash
   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
   source ~/.cargo/env
   ```

2. **å®‰è£… WebAssembly target**ï¼š
   ```bash
   rustup target add wasm32-wasip2
   ```

3. **å®‰è£… Python ä¾èµ–**ï¼ˆå¯é€‰ï¼Œç”¨äºåç«¯ï¼‰ï¼š
   ```bash
   # å¦‚æœä½¿ç”¨ Python åç«¯
   cd backend/backend-python
   pip install -r requirements.txt
   cd ../..
   ```

## ğŸ”§ æ‰‹åŠ¨è¿è¡Œæ­¥éª¤

å¦‚æœä½ æƒ³æ‰‹åŠ¨æ§åˆ¶æ¯ä¸ªæ­¥éª¤ï¼š

### 1. ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
```bash
# ä¸‹è½½ llama-3.2-1b-instruct æ¨¡å‹
cd pie-cli
cargo run -- model add "llama-3.2-1b-instruct"
cd ..
```

### 2. ç¼–è¯‘ Simple Test
```bash
cd example-apps
cargo build --target wasm32-wasip2 --release
cd ..
```

### 3. å¯åŠ¨ PIE æœåŠ¡å™¨
```bash
cd pie-cli
export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
export TORCH_CUDA_ARCH_LIST="8.9"
pie start --config ./example_config.toml
```

### 4. è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆåœ¨æ–°ç»ˆç«¯ï¼‰
```bash
cd /home/neiwen/pie && python benchmarks/test_simple_test_multiple.py 

# åŸºæœ¬æµ‹è¯•ï¼ˆ10ä¸ªå®ä¾‹ï¼‰
python test_simple_test_multiple.py

# å¯ç”¨å¥å·æš‚åœæ¨¡å¼
python test_simple_test_multiple.py --semicolon-pause --verbose

# å¤§è§„æ¨¡æµ‹è¯•
python test_simple_test_multiple.py --num-instances 50 --max-tokens 200
```

## ğŸ“Š æµ‹è¯•ç»“æœ

æµ‹è¯•å®Œæˆåï¼Œç»“æœä¼šä¿å­˜åˆ° `logs/test_simple_test_multiple.json`ï¼ŒåŒ…å«ï¼š
- æ€»ç”¨æ—¶
- ååé‡ï¼ˆè¯·æ±‚/ç§’ï¼‰
- æ¨¡å‹ä¿¡æ¯
- å¥å·æš‚åœæ¨¡å¼çŠ¶æ€

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç¼–è¯‘å¤±è´¥**ï¼š
   ```bash
   # æ¸…ç†å¹¶é‡æ–°ç¼–è¯‘
   cd example-apps
   cargo clean
   cargo build --target wasm32-wasip2 --release
   ```

2. **æœåŠ¡å™¨è¿æ¥å¤±è´¥**ï¼š
   - æ£€æŸ¥ç«¯å£ 8080 æ˜¯å¦è¢«å ç”¨
   - ç¡®è®¤æœåŠ¡å™¨å·²ç»å®Œå…¨å¯åŠ¨ï¼ˆéœ€è¦å‡ ç§’é’Ÿï¼‰

3. **æ¨¡å‹ä¸‹è½½å¤±è´¥**ï¼š
   ```bash
   # æ‰‹åŠ¨æ¸…ç†æ¨¡å‹ç¼“å­˜
   rm -rf ~/.cache/pie/models/llama-3.2-1b-instruct*
   # é‡æ–°ä¸‹è½½
   cd pie-cli && cargo run -- model add "llama-3.2-1b-instruct"
   ```

4. **Python ä¾èµ–é—®é¢˜**ï¼š
   ```bash
   # é‡æ–°å®‰è£…ä¾èµ–
   cd backend/backend-python
   pip install --upgrade -r requirements.txt
   ```

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹ PIE æœåŠ¡å™¨æ—¥å¿—
tail -f pie-cli/pie.log

# æŸ¥çœ‹ Python åç«¯æ—¥å¿—
tail -f ~/.cache/pie/logs/backend.log
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `pie-cli/example_config.toml` æ¥è°ƒæ•´ï¼š

```toml
# æœåŠ¡å™¨é…ç½®
host = "127.0.0.1"
port = 8080

# æ‰¹å¤„ç†ç­–ç•¥
batching_strategy = "adaptive"  # adaptive, k, t, kort

# åç«¯é…ç½®
[[backend]]
backend_type = "python"
model = "llama-3.2-1b-instruct"
device = "cuda:0"  # æˆ– "cpu"
```

### è‡ªå®šä¹‰åŸºå‡†æµ‹è¯•å‚æ•°

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°
python benchmarks/test_simple_test_multiple.py --help
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨ GPU**ï¼šç¡®ä¿ `device = "cuda:0"` åœ¨é…ç½®æ–‡ä»¶ä¸­
2. **è°ƒæ•´æ‰¹å¤„ç†**ï¼šå°è¯•ä¸åŒçš„ `batching_strategy`
3. **å¢åŠ å†…å­˜**ï¼šå¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ï¼Œå‡å°‘ `num-instances`
4. **ç›‘æ§èµ„æº**ï¼šä½¿ç”¨ `nvidia-smi` æˆ– `htop` ç›‘æ§ç³»ç»Ÿèµ„æº

## ğŸ”— æ›´å¤šèµ„æº

- [PIE é¡¹ç›®ä¸»é¡µ](https://pie-project.org)
- [æ¨¡å‹ç´¢å¼•](https://github.com/pie-project/model-index)
- [å®Œæ•´æ–‡æ¡£](https://pie-project.org/docs/)

---

ğŸ‰ **ç°åœ¨ä½ å¯ä»¥å¼€å§‹æµ‹è¯• PIE ç³»ç»Ÿçš„æ€§èƒ½äº†ï¼**

